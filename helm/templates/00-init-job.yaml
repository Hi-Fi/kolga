{{- if .Values.application.initializeCommand -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ template "trackableappname" . }}-initialize
  namespace: {{ .Values.namespace }}
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-weight": "1"
  labels:
    {{- include "commonLabels" . | nindent 4 }}
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        {{- include "commonLabels" . | nindent 8 }}
    spec:
      restartPolicy: Never
      imagePullSecrets:
        - name: regcred
      containers:
      - name: "{{ .Chart.Name }}"
        image: "{{ .Values.image }}"
        command: ["/bin/sh"]
        args: ["-c", "{{ .Values.application.initializeCommand }}"]
        imagePullPolicy: {{ .Values.app_image.pullPolicy }}
        # This is a temporary work around until we get a better solution
        # for solving HPA matching on both the deployment and the jobs.
        # A proper solution here would be that the Deployment would not
        # match the Job at all using extra labels on the Deployment (breaking change).
        # This workaround works as HPA requires there to be resource requests.
        # The major downside here is that this can have unforeseen side-effects
        # to the actual scaling.
        resources:
          requests:
            cpu: "1m"
            memory: "1Mi"
        {{- if .Values.application.secretName }}
        envFrom:
        - secretRef:
            name: {{ .Values.application.secretName }}
        {{- end }}
        {{- if .Values.application.fileSecretName }}
        volumeMounts:
        {{- if .Values.application.fileSecretName }}
          - mountPath: {{ .Values.application.fileSecretPath }}
            name: filesecrets
        {{- end }}
        {{- end }}
      {{- if .Values.application.fileSecretName }}
      volumes:
      {{- if .Values.application.fileSecretName }}
        - name: filesecrets
          secret:
            secretName: {{ .Values.application.fileSecretName }}
      {{- end }}
      {{- end }}
{{- end -}}
